{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HydroDL","text":"<p>HydroDL is a coding library designed to model hydrologic systems using two classes of models: (1) purely data-driven neural networks and (2) hybrid, physics-informed differentiable (\\(\\delta\\)) models. Applications range from soil moisture to streamflow to water quality and ecosystem. Through this portal, we collect all of our published models so you can navigate through them easily. These codes are developed by the Multiscale Hydrology, Processes and Intelligence (MHPI) group headed by Dr. Chaopeng Shen, faculty member at Penn State Univ (http://water.engr.psu.edu/shen). Codes are contributed by the whole MHPI team (https://sites.google.com/view/mhpi/team).</p> <p>The two model classes each have their own advantages. Purely data-driven models are great because they are: (i) easy to train and run at scale. (ii) automatically adaptable to predictable errors in forcings (iii) highly efficient in computation. (iv) not heavily prone to human bias in model selection.  </p> <p>Differentiable models mix process-based equations (called priors) and neural networks (NNs) at the fundamental level so they can be trained together in one stage (called \"end-to-end\"). This way, the network components can be supervised indirectly by outputs of the combined system, and do not necessarily need supervising data for its direct outputs. Differentiability can be supported by automatic differentiation, adjoint (Song et al., 2023, https://hess.copernicus.org/preprints/hess-2023-258/), or any other method that can produce the gradients of loss with respect to large amounts of parameters efficiently. Such models can train one neural network using big data, improving the generalizability, robustness, and complexity of the learned relationships.   </p> <p>While being able to match purely data-driven models in performance in even data-dense regions, differentiable models have the following advantages: (1) Interpretable: they represent the full physical processes and output intermediate physical variables not used in training, e.g., ET, recharge, snow water equivalent, which help to provide an explainable narrative. (2) Generalizability: because of the use of priors, differentiable models often generalize better under data-sparse regions like in global hydrologic simulations. (Feng et al., 2023a, https://doi.org/10.5194/hess-27-2357-2023; 2023b, https://gmd.copernicus.org/preprints/gmd-2023-190/) (3) (Under investigation) Extremes: forced by physical priors, the differentiable models may be better at representing unseen extreme scenarios and future trends. (4) Knowledge Discovery: we can use NNs as question marks in the combined system to learn unknown/uncertain relationships from data. In this sense, the priors serve to constrain or narrow down the scope of learning so we can understand the learned relationships. (5) Respect physical laws and sensitivities: we can force the model to respect laws such as mass balance, energy-driven evapotranspiration, etc. so our model has the correct sensitivities.</p>"},{"location":"Contribute/","title":"Contribute","text":""},{"location":"Contribute/#tbd","title":"TBD","text":""},{"location":"Example/","title":"Examples","text":"<p>Several examples related to the above papers are presented here. Click the title link to see each example.</p>"},{"location":"Example/#1train-a-lstm-data-integration-model-to-make-streamflow-forecast","title":"1.Train a LSTM data integration model to make streamflow forecast","text":"<p>The dataset used is NCAR CAMELS dataset. Download CAMELS following this link.  Please download both forcing, observation data <code>CAMELS time series meteorology, observed flow, meta data (.zip)</code> and basin attributes <code>CAMELS Attributes (.zip)</code>.  Put two unzipped folders under the same directory, like <code>your/path/to/Camels/basin_timeseries_v1p2_metForcing_obsFlow</code>, and <code>your/path/to/Camels/camels_attributes_v2.0</code>. Set the directory path <code>your/path/to/Camels</code> as the variable <code>rootDatabase</code> inside the code later.</p> <p>Computational benchmark: training of CAMELS data (w/ or w/o data integration) with 671 basins, 10 years, 300 epochs, in ~1 hour with GPU.</p>"},{"location":"Installation/","title":"Installation","text":""},{"location":"Installation/#installation","title":"Installation","text":"<p>There are two different methods for hydroDL installation:</p>"},{"location":"Installation/#create-a-new-environment-then-activate-it","title":"Create a new environment, then activate it","text":"<pre><code>conda create -n mhpihydrodl python=3.10\nconda activate mhpihydrodl\n</code></pre>"},{"location":"Installation/#using-pypi-stable-package","title":"Using PyPI (stable package)","text":"<p>Install our hydroDL stable package from pip (Python version&gt;=3.0)</p> <pre><code>pip install hydroDL\n</code></pre>"},{"location":"Installation/#source-latest-version","title":"Source latest version","text":"<p>Install our latest hydroDL package from github</p> <pre><code>git clone https://github.com/mhpi/hydroDL.git\n</code></pre> <p>Note: If you want to run our examples directly, please download the example folder (It contains the code and data for these examples). </p> <p>There exists a small compatibility issue with our code when using the latest pyTorch version. Feel free to contact us if you find any issues or code bugs that you cannot resolve.</p>"},{"location":"Quick_start/","title":"Quick Start:","text":"<p>The detailed code for quick start can be found in tutorial_quick_start.py</p> <p>See below for a brief explanation of the major components you need to run a hydroDL model:</p> <pre><code># imports\nfrom hydroDL.model.crit import RmseLoss\nfrom hydroDL.model.rnn import CudnnLstmModel as LSTM\nfrom hydroDL.model.train import trainModel\nfrom hydroDL.model.test import testModel\n\n# load your training and testing data \n# x: forcing data (pixels, time, features)\n# c: attribute data (pixels, features)\n# y: observed values (pixels, time, 1)\nx_train, c_train, y_train, x_val, c_val, y_val = load_data(...)\n\n# define your model and loss function\nmodel = LSTM(nx=num_variables, ny=1)\nloss_fn = RmseLoss()\n\n# train your model\nmodel = trainModel(model,\n    x_train,\n    y_train,\n    c_train,\n    loss_fn,\n)\n\n# validate your model\npred = testModel(model,\n             x_val,\n             c_val,\n)\n</code></pre>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>We here provide comparisons to LSTM models on the CAMELS data (top of page) as well as comparisons to the current National Water Model at the national scale (bottom of this page), and more comparisons will be provided here.</p> <p>We recently updated our LSTM, and you can find the high-flow expert on hydroDL repo's tutorial (see Codes tab on this website). The first and forecast benchmark is over the CAMELS dataset. The results can vary slightly due to training/test periods. Below you will find results for 10-year training (exactly as reported in Kratzert et al., 2019) and 15-year training (shown in this Figure). Besides NSE and KGE, we also report absolute FHV and FLV (these metrics have + or - signs, and they make more sense after taking the absolute sign) and low-flow and high-flow RMSE. So far, the best LSTM is LSTM-hydroDL (high-flow expert) and the best differentiable model is \\(\\delta\\)HBV.adjoint (https://hess.copernicus.org/preprints/hess-2023-258/). As time goes on, we will also report benchmarks on the global dataset and other papers. We also know that spatial test (trained on some basins, tested on some other basins) or prediction in ungauged regions (PUR) tests (tested in a large region without training data) are more stringent tests and will likely change the comparisons. We previously found differentiable model to perform better in the PUR test (Feng et al., 2023 https://doi.org/10.5194/hess-27-2357-2023).</p>"},{"location":"benchmarks/#cdf-comparison","title":"CDF Comparison","text":"Camels NSE of popular streamflow models (single, without ensemble) wth 15-year training. This is a temporal test (trained on ). We compared 3 versions of differentiable HBV model (\"Unmodified\"-- without any structural update; $\\delta$HBV -- a sequential differentiable HBV published in Feng et al., 2022; and $\\delta$HBV.adjoint, slightly modified from Song et al., 2023. See refs below) with two versions of hydroDL implementation (a high-flow expert and a low-flow expert). We also trained the LSTM from Kratzert 2019 for comparison."},{"location":"benchmarks/#metric-tables","title":"Metric Tables","text":""},{"location":"benchmarks/#10-year-training-comparison","title":"10-year training comparison","text":"<p>Info</p> <p>All models were trained from 1999/10/01 to 2008/09/30 and tested from 1989/10/01 to 1999/09/30 on the subset of 531 CAMELS basins. </p> Model Median NSE Median KGE Median Absolute (Non-Absolute) FLV (%) Median Absolute (Non-Absolute) FHV (%) Median low flow RMSE (mm/day) Median peak flow RMSE (mm/day) LSTM-hydroDL-single (high-flow expert)\u202f 0.74 0.76 31.79 (-9.08) 16.20 (-13.42) 0.049 3.28 LSTM-hydroDL-Ensemble (high-flow expert)\u202f 0.765 0.77 28.84 (-3.88) 16.21 (-13.38) 0.046 3.27 LSTM-single ran w/ code in Kratzert et al. (2019) 0.74 0.75 32.02 (5.54) 18.02 (-15.80) 0.051 3.70 LSTM-single (Kratzert et al. 2019) As reported\u202f 0.731 - - (26.5) - (-14.8) - - LSTM-Ensemble (Kratzert et al. 2019) As reported\u202f 0.758 - - (55.1) - (-15.7) - -"},{"location":"benchmarks/#15-year-training-comparison","title":"15-year training comparison","text":"<p>Info</p> <p>All models were trained from 1980/10/01 to 1995/09/30 and tested from 1995/10/01 to 2010/09/30 on all 671 CAMELS basins.</p> Model Median NSE Median KGE Median Absolute (Non-Absolute) FLV (%) Median Absolute (Non-Absolute) FHV (%) Median low flow RMSE (mm/day) Median peak flow RMSE (mm/day) Baseflow index\u202fspatial correlation Median NSE of temporal ET simulation LSTM-hydroDL (low-flow expert) 0.73 0.76 19.52 (12.21) 15.01 (-4.12) 0.023 2.67 - - LSTM-hydroDL (high-flow expert)\u202f 0.74 0.78 37.33 (-20.72) 13.68 (-4.30) 0.048 2.49 - - LSTM ran w/ code in Kratzert et al. (2019) 0.73 0.77 40.59 (29.70) 13.46  (-4.19) 0.055 2.56 - - SAC-SMA (Traditional) 0.66 0.73 59.40 (46.96) 17.55 (-9.79) 0.081 3.19 - - Unmodified \\(\\delta\\)HBV 0.69 0.72 47.58 (16.84) 16.40 (-10.80) 0.066 2.74 0.76 0.43 \\(\\delta\\)HBV 0.73 0.73 56.53 (50.93) 15.29 (-8.89) 0.074 2.56 0.76 0.59 \\(\\delta\\)HBV.adj (expert 1) 0.72 0.75 43.29 (37.61) 13.25 (-4.33) 0.048 2.47 0.83 0.61 \\(\\delta\\)HBV.adj (expert 2) 0.75 0.76 40.56 (32.78) 14.09 (-7.97) 0.045 2.59 0.87 0.62"},{"location":"benchmarks/#citations","title":"Citations","text":"LSTM-hydroDL (low-flow expert)LSTM-hydroDL (high-flow expert)LSTM (Kratzert et al. 2019)SAC-SMA (Traditional)Unmodified \\(\\delta\\) HBV\\(\\delta\\) HBV\\(\\delta\\) HBV.adj <pre><code>Feng, Dapeng, Kuai Fang, and Chaopeng Shen. \"Enhancing streamflow forecast and extracting \ninsights using long\u2010short term memory networks with data integration at continental scales.\n\" Water Resources Research 56, no. 9 (2020): e2019WR026793. \n</code></pre> <pre><code>Github link:\n</code></pre> https://https://neuralhydrology.github.io/<pre><code>Kratzert, Frederik, Daniel Klotz, Guy Shalev, G\u00fcnter Klambauer, Sepp Hochreiter, and Grey \nNearing. \"Benchmarking a catchment-aware long short-term memory network (LSTM) for\nlarge-scale hydrological modeling.\" Hydrol. Earth Syst. Sci. Discuss 2019 (2019): 1-32.\n</code></pre> <pre><code>Newman, Andrew J., Martyn P. Clark, Kevin Sampson, Andrew Wood, Lauren E. Hay, Andy Bock, \nRoland J. Viger et al. \"Development of a large-sample watershed-scale hydrometeorological \ndata set for the contiguous USA: data set characteristics and assessment of regional \nvariability in hydrologic model performance.\" Hydrology and Earth System Sciences 19, no. 1 \n(2015): 209-223. \n</code></pre> <pre><code>Feng, D., Liu, J., Lawson, K. and Shen, C., 2022. Differentiable, learnable, regionalized \nprocess\u2010based models with multiphysical outputs can approach state\u2010of\u2010the\u2010art hydrologic \nprediction accuracy. Water Resources Research, 58(10), p.e2022WR032404. \n</code></pre> <pre><code>Feng, D., Liu, J., Lawson, K. and Shen, C., 2022. Differentiable, learnable, regionalized \nprocess\u2010based models with multiphysical outputs can approach state\u2010of\u2010the\u2010art hydrologic \nprediction accuracy. Water Resources Research, 58(10), p.e2022WR032404.\n</code></pre> <pre><code>Song, Yalan, Wouter Knoben, Martyn P. Clark, Dapeng Feng, Kathryn Lawson, and Chaopeng \nShen, When ancient numerical demons meet physics-informed machine learning: adjoint-based \ngradients for implicit differentiable modeling \nPreprint link: https://hess.copernicus.org/preprints/hess-2023-258/ \n</code></pre>"},{"location":"benchmarks/#comparison-with-national-water-models","title":"Comparison with National Water Models","text":"<p>Funded by CIROH projects, we have produced initial comparisons at the continental scale showing the superior performance of the differentiable models compared to both NOAA\u2019s first-generation WRF-Hydro.NWM Model, version 1.2 (Tijerina\u2010Kreuzer et al., 2021) and version 2.1 (Cosgrove et al., 2024). The differentiable routing model developed in our FY22 CIROH project is used for runoff routing using Muskingum-Cunge method. We are now producing seamless streamflow simulations at high spatial resolution for the whole CONUS and the results below are demonstrating one of the simulations. We are still improving the runoff, forcing, and routing aspects of the product. Several updates are incoming. Please stand by for a data release!</p> <p> </p> Condon diagrams comparing streamflow performance for $\\delta$HBV -- differentiable HBV and National Water Model, version 1.2. The $\\delta$HBV is trained from 10/1980 to 09/1995 and tested from 01/1981 to 12/2019. NWM Model, version 1.2 is uncalibrated and tested from 10/1984 to 09/1985 (reprinted from Tijerina\u2010Kreuzer et al., 2021)   <p> </p> Streamflow normalized Nash Sutcliffe Efficiency (NNSE) and correlation comparison between $\\delta$HBV -- differentiable HBV and NWM Model, version 2.1. The $\\delta$HBV is trained from 10/1980 to 09/1995 and tested from 01/1981 to 12/2019. NWM Model, version 2.1 is calibrated from 10/2008 to 09/2013 and tested from 10/2013 to 09/2016 (reprinted from Cosgrove et al., 2024)."},{"location":"benchmarks/#ensemble-performances","title":"Ensemble performances","text":"<p>Significant benefits were achieved by ensembling \u03b4HBV and LSTM models across temporal, PUB (Prediction in Ungauged Basin), and PUR (Prediction in Ungauged region) tests. All models were trained from October 1, 1980 to September 30, 1995 on all 671 CAMELS basins, and tested from October 1, 1995 to September 30, 2008 on a selected set of 531 basins.</p> <p></p>   Median NSE values for 531 CAMELS basins, indicating model and ensemble performances for (a) temporal, (b) prediction in ungauged basin (PUB), and (c) prediction in ungauged region (PUR) tests. Different simulations are represented by variously-shaped and -colored points, and are organized by ensemble group, listed along the x-axis: LSTM, \u03b4HBV, LSTM+\u03b4HBV, and their \u201censemble forcing\u201d counterparts, LSTM<sup>ef</sup>, \u03b4HBV<sup>ef</sup>, and LSTM+\u03b4HBV<sup>ef</sup>. LSTM<sup>multi</sup> is a single LSTM model trained directly on all three forcing datasets at once. The superscript \u201cef\u201d denotes the forcing datasets involved in each ensemble (choices of 1 for Daymet, 2 for NLDAS, and 3 for Maurer), while the \u201c+\u201d connects the model types used within an ensemble. The x-axis group and subscript \u201cseed\u201d indicate that simulation results were averaged based on three different random seeds (see Figure C1). Other points without \u201cseed\u201d, along with their corresponding error bars, are derived from the averages of metrics computed over repeated runs with three different random seeds. The error bar indicates one standard deviation above and below the average value for each simulation.    <p>Citation: Li, P., Song, Y., Pan, M., Lawson, K., &amp; Shen, C. (2025). Ensembling Differentiable Process-based and Data-driven Models with Diverse Meteorological Forcing Datasets to Advance Streamflow Simulation. EGUsphere, 2025, 1\u201374. https://doi.org/10.5194/egusphere-2025-483</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"codes/","title":"Code","text":"<p>See below for coding projects developed by the community that utilize HydroDL</p>"},{"location":"codes/#differentiable-modeling-framework","title":"Differentiable Modeling Framework","text":"<ul> <li>\ud835\udeffMG (Lonzarich et al. 2024)     ---  A second-generation generic, scalable differentiable modeling framework on PyTorch for integrating neural networks with physical models. Coupled with HydroDL2, \ud835\udeffMG enables hydrologic modeling like HydroDL while greatly expanding the range of applications and capabilities.</li> </ul>"},{"location":"codes/#differentiable-models","title":"Differentiable Models","text":"<ul> <li> <p>\\(\\delta\\) MC-Juniata-hydroDL2 (Bindas et al. 2024)</p> <p> A differentiable routing method that uses Muskingum-Cunge and an NN to infer parameterizations for Manning\u2019s roughness</p> </li> </ul> <ul> <li> <p>\\(\\delta\\) differentiable stream temperature (LSTM + SNTEMP) (Rahmani et al. 2023)</p> <p> CONUS scale pearson correlation between baseflow estimation of our model and GAGES-II estimates of baseflow.</p> </li> </ul> <ul> <li> <p>diffEcosys (Aboelyazeed_et_al_2023)</p> <p> A differentiable physics-informed ecosystem model </p> </li> </ul> <ul> <li> <p>\\(\\delta\\) HBV-hydroDL (Feng et al., 2022,2023)</p> <p> Differentiable hydrologic models using HBV model as a physical backbone.</p> </li> </ul> <ul> <li> <p>\\(\\delta\\) parameter learning (Tsai_et_al_2021)</p> <p> Differentiable Parameter Learning</p> </li> </ul> <ul> <li> <p>Updated, simplified \\(\\delta\\)HBV tutorial for CAMELS streamflow</p> <p>This notebook is a simplified \\(\\delta\\)HBV code tutorial for CAMELS streamflow. Thanks to Yalan Song and Dapeng Feng. Note that different pytorch versions could lead to slightly different performances. </p> </li> </ul>"},{"location":"codes/#lstm-models","title":"LSTM Models","text":"<ul> <li> <p>Starting point: Quick LSTM tutorial on soil moisture prediction</p> <p>This notebook is the starting point for new people to learn hydrologic time series prediction using deep neural networks. You can see how CudnnLSTMmodel and CpuLSTM models are trained and how data are formatted. Dataset is embedded in the hydroDL library so it is easy to run the example.</p> </li> </ul> <ul> <li> <p>Updated, simplified LSTM tutorial for CAMELS streamflow</p> <p>This notebook is the \"high-flow expert\" listed on the benchmark page. We greatly simplified the LSTM interface, making it easy to reuse this code on your data. This is slightly more involved than the soil moisture tutorial as we are dealing with a larger and more complex dataset. Thanks to Yalan Song, Kamlesh Sawadekar and Dapeng Feng. Note that different pytorch versions could lead to slightly different performances. </p> </li> </ul> <ul> <li> <p>Multiscale (Liu et al. 2022)</p> <p> A multiscale DL scheme learning simultaneously from satellite and in situ data to predict 9 km daily soil moisture (5 cm depth). </p> </li> </ul> <ul> <li> <p>LSTM for snow water equivalent (Song et al. 2023)</p> <p> Time series of the forward model and the models integrating snow water equivalent (SWE) and snow cover fraction (SCF) at different time lags. </p> </li> </ul> <ul> <li> <p>LSTM stream temperature model (Rahmani et al. 2021)</p> <p> CONUS scale aggregated metrics of stream temperature models for the testing time range considering observed, simulated, and no streamflow data among inputs,with a locally fitted auto-regressive model</p> </li> </ul>"},{"location":"codes/#transformer-models","title":"Transformer Models","text":"<ul> <li> <p>Transformer (Liu et al. 2024)</p> <p> First time Transformer achieved the same performance as LSTM on CAMELS dataset; LSTMs and Transformers are likely nearing the prediction limits of the dataset.</p> </li> </ul>"},{"location":"codes/Aboelyazeed_2023/","title":"diffEcosys","text":""},{"location":"codes/Aboelyazeed_2023/#short-summary","title":"Short Summary","text":"<p>Photosynthesis is critical for life and has been affected by the changing climate. Many parameters come into play while modeling, but traditional calibration approaches face many issues. Our framework trains coupled neural networks to provide parameters to a photosynthesis model. Using big data, we independently found parameter values that were correlated with those in the literature while giving higher correlation and reduced biases in photosynthesis rates.</p>"},{"location":"codes/Aboelyazeed_2023/#code-release","title":"Code Release","text":"<p>github version: https://github.com/hydroPKDN/diffEcosys/</p> <p>zenodo version: https://zenodo.org/records/8067204</p>"},{"location":"codes/Aboelyazeed_2023/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{Aboelyazeed2023,\n  doi = {10.5194/bg-20-2671-2023},\n  url = {https://doi.org/10.5194/bg-20-2671-2023},\n  year = {2023},\n  month = jul,\n  publisher = {Copernicus {GmbH}},\n  volume = {20},\n  number = {13},\n  pages = {2671--2692},\n  author = {Doaa Aboelyazeed and Chonggang Xu and Forrest M. Hoffman and Jiangtao Liu and Alex W. Jones and Chris Rackauckas and Kathryn Lawson and Chaopeng Shen},\n  title = {A differentiable,  physics-informed ecosystem modeling and  learning framework for large-scale inverse problems:  demonstration with photosynthesis simulations},\n  journal = {Biogeosciences}\n}\n</code></pre>"},{"location":"codes/Rahmani_et_al_2021/","title":"LSTM stream temperature model","text":""},{"location":"codes/Rahmani_et_al_2021/#code-release","title":"Code Release","text":"<p>See here for the USGS release</p>"},{"location":"codes/Rahmani_et_al_2021/#results","title":"Results","text":"<p>We developed a basin-centric LSTM model for daily stream temperature prediction and to evaluate the impact of streamflow information on the predictions.</p>"},{"location":"codes/Rahmani_et_al_2021/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{Rahmani2021lstm,\n  title={Exploring the Exceptional Performance of a Deep Learning Stream Temperature Model and the Value of Streamflow Data},\n  author={Rahmani, Farshid and Lawson, Kathryn and Ouyang, Wenyu and Appling, Alison and Oliver, Samantha and Shen, Chaopeng},\n  journal={Environmental Research Letters},\n  year={2021},\n  publisher={IOPScience}\n}\n</code></pre>"},{"location":"codes/Rahmani_et_al_2023/","title":"\\(\\delta\\) differentiable stream temperature (LSTM + SNTEMP)","text":""},{"location":"codes/Rahmani_et_al_2023/#code-data-models-and-results-release","title":"Code, data, models, and results Release","text":"<p>See here for the USGS release</p>"},{"location":"codes/Rahmani_et_al_2023/#summary","title":"Summary","text":"<p>We integrated a process-based stream water temperature model (SNTEMP) with LSTM module in a differentiable platform (PyTorch) to predict dily stream water temperature. The main difference between the differentiable model and many other previous physics-guided frameworks is that the process-based model is written in a differentiable platform like those used for NNs. This approach makes it possible for the framework to seamlessly integrate both machine learning and process-based model parts and be trained as a unified model (Shen et al., 2023). The training process is \u201cend-to-end,\u201d with any parameters within the model being learnable by gradient descent, just like when a pure LSTM model is trained. Thus, there is no need to calibrate each component independently. In other words, the process-based model becomes a part of the NN with the added advantage of process transparency.. All the hybrid models were trained on daily water temperature observations for 415 stations across the conterminous United States (CONUS). We evaluated the model with two metrics: the daily temperature simulation accuracy against observed data, and the correlation between the baseflow estimation of our model with a published alternative estimate based on baseflow recession analysis. </p>"},{"location":"codes/Rahmani_et_al_2023/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{Rahmani2021lstm,\n  title={Identifying Structural Priors in a Hybrid Differentiable Model for Stream Water Temperature Modeling},\n  author={Rahmani, Farshid and Appling, Alison and Feng, Dapeng and Lawson, Kathryn and Shen, Chaopeng},\n  journal={Water Resources Research},\n  year={2023},\n  publisher={AGU}\n}\n</code></pre>"},{"location":"codes/Song_SWE_2023/","title":"LSTM for snow water equivalent","text":""},{"location":"codes/Song_SWE_2023/#code-release","title":"Code Release","text":"<p>LSTM code for SWE is available at this link: https://github.com/mhpi/hydroDL/tree/release/example/snow_water_equivalent</p>"},{"location":"codes/Song_SWE_2023/#short-summary","title":"Short Summary","text":"<p>We tested an LSTM network with data integration (DI) for snow water equivalent (SWE) in the western US to integrate 30-day-lagged or 7-day-lagged observations of either SWE or satellite-observed snow cover fraction (SCF) to improve future predictions. SCF proved beneficial only for shallow-snow sites during snowmelt, while lagged SWE integration significantly improved prediction accuracy for both shallow-and deep-snow sites. The median Nash-Sutcliffe model efficiency coefficient (NSE) in temporal testing improved from 0.92 to 0.97 with 30-day-lagged SWE integration, and root-mean-square error (RMSE) and the difference between estimated and observed peak SWE values (dmax ) were reduced by 41% and 57%, respectively (https://journals.ametsoc.org/view/journals/hydr/25/1/JHM-D-22-0220.1.xml).</p>"},{"location":"codes/Song_SWE_2023/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{song2024lstm,\n  title={LSTM-based data integration to improve snow water equivalent prediction and diagnose error sources},\n  author={Song, Yalan and Tsai, Wen-Ping and Gluck, Jonah and Rhoades, Alan and Zarzycki, Colin and McCrary, Rachel and Lawson, Kathryn and Shen, Chaopeng},\n  journal={Journal of Hydrometeorology},\n  volume={25},\n  number={1},\n  pages={223--237},\n  year={2024},\n  publisher={American Meteorological Society}\n}\n</code></pre>"},{"location":"codes/Tsai_2021/","title":"\\(\\delta\\) parameter learning","text":""},{"location":"codes/Tsai_2021/#code-release","title":"Code Release","text":"<ul> <li>Codes are released at this zenodo link</li> </ul>"},{"location":"codes/Tsai_2021/#summary","title":"Summary","text":"<p>From calibration to parameter learning: Harnessing the scaling effects of big data in geoscientific modeling for routing flows on the river network.</p> <p>The behaviors and skills of models in many geosciences (e.g., hydrology and ecosystem sciences) strongly depend on spatially-varying parameters that need calibration.  A well-calibrated model can reasonably propagate information from observations to unobserved variables via model physics, but traditional calibration is highly inefficient  and results in non-unique solutions. Here we propose a novel differentiable parameter learning (dPL) framework that efficiently learns a global mapping between inputs  (and optionally responses) and parameters. Crucially, dPL exhibits beneficial scaling curves not previously demonstrated to geoscientists: as training data increases,  dPL achieves better performance, more physical coherence, and better generalizability (across space and uncalibrated variables), all with orders-of-magnitude lower  computational cost. We demonstrate examples that learned from soil moisture and streamflow, where dPL drastically outperformed existing evolutionary and regionalization methods, or required only ~12.5% of the training data to achieve similar performance. The generic scheme promotes the integration of deep learning and process-based models, without mandating reimplementation.</p>"},{"location":"codes/Tsai_2021/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{Tsai_2021,\n   title={From calibration to parameter learning: Harnessing the scaling effects of big data in geoscientific modeling},\n   volume={12},\n   ISSN={2041-1723},\n   url={http://dx.doi.org/10.1038/s41467-021-26107-z},\n   DOI={10.1038/s41467-021-26107-z},\n   number={1},\n   journal={Nature Communications},\n   publisher={Springer Science and Business Media LLC},\n   author={Tsai, Wen-Ping and Feng, Dapeng and Pan, Ming and Beck, Hylke and Lawson, Kathryn and Yang, Yuan and Liu, Jiangtao and Shen, Chaopeng},\n   year={2021},\n   month=oct }\n</code></pre>"},{"location":"codes/bindas_2023/","title":"\\(\\delta\\) MC-Juniata-hydroDL2","text":""},{"location":"codes/bindas_2023/#code-release","title":"Code Release","text":"<p>Below are Zenodo releases for:</p> <ul> <li>Code</li> <li>Dataset</li> </ul> <p>Check out the code on Github here</p>"},{"location":"codes/bindas_2023/#summary","title":"Summary","text":"<p>A novel differentiable modeling framework to perform routing and to learn a \u201cparameterization scheme\u201d (a systematic way of inferring parameters from more rudimentary information) for routing flows on the river network.</p> <p>Below are the main points from our paper:</p> <ul> <li>A novel differentiable routing model can learn effective river routing parameterization, recovering channel roughness in synthetic runs.</li> <li>With short periods of real training data, we can improve streamflow in large rivers compared to models not considering routing.</li> <li>For basins &gt;2,000 km2, our framework outperformed deep learning models that assume homogeneity, despite bias in the runoff forcings.</li> </ul>"},{"location":"codes/bindas_2023/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{https://doi.org/10.1029/2023WR035337,\nauthor = {Bindas, Tadd and Tsai, Wen-Ping and Liu, Jiangtao and Rahmani, Farshid and Feng, Dapeng and Bian, Yuchen and Lawson, Kathryn and Shen, Chaopeng},\ntitle = {Improving River Routing Using a Differentiable Muskingum-Cunge Model and Physics-Informed Machine Learning},\njournal = {Water Resources Research},\nvolume = {60},\nnumber = {1},\npages = {e2023WR035337},\nkeywords = {flood, routing, deep learning, physics-informed machine learning, Manning's roughness},\ndoi = {https://doi.org/10.1029/2023WR035337},\nurl = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2023WR035337},\neprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2023WR035337},\nnote = {e2023WR035337 2023WR035337},\nabstract = {Abstract Recently, rainfall-runoff simulations in small headwater basins have been improved by methodological advances such as deep neural networks (NNs) and hybrid physics-NN models\u2014particularly, a genre called differentiable modeling that intermingles NNs with physics to learn relationships between variables. However, hydrologic routing simulations, necessary for simulating floods in stem rivers downstream of large heterogeneous basins, had not yet benefited from these advances and it was unclear if the routing process could be improved via coupled NNs. We present a novel differentiable routing method (\u03b4MC-Juniata-hydroDL2) that mimics the classical Muskingum-Cunge routing model over a river network but embeds an NN to infer parameterizations for Manning's roughness (n) and channel geometries from raw reach-scale attributes like catchment areas and sinuosity. The NN was trained solely on downstream hydrographs. Synthetic experiments show that while the channel geometry parameter was unidentifiable, n can be identified with moderate precision. With real-world data, the trained differentiable routing model produced more accurate long-term routing results for both the training gage and untrained inner gages for larger subbasins (&gt;2,000\u00a0km2) than either a machine learning model assuming homogeneity, or simply using the sum of runoff from subbasins. The n parameterization trained on short periods gave high performance in other periods, despite significant errors in runoff inputs. The learned n pattern was consistent with literature expectations, demonstrating the framework's potential for knowledge discovery, but the absolute values can vary depending on training periods. The trained n parameterization can be coupled with traditional models to improve national-scale hydrologic flood simulations.},\nyear = {2024}\n}\n</code></pre>"},{"location":"codes/feng_2023/","title":"\\(\\delta\\) HBV-hydroDL","text":""},{"location":"codes/feng_2023/#code-release","title":"Code Release","text":"<p>The codes for differentiable hydrologic models are released at this zenodo link</p>"},{"location":"codes/feng_2023/#summary","title":"Summary","text":"<p>Differentiable hydrologic modeling provides a generic approach to unify machine learning and physical models. Here we first implemented the hydrologic model HBV in the PyTorch platform with Automatic Differentiation, so that we can freely embed neural network components to parameterize and evolve the structure of the original model. We flexibly evolved the original HBV structure under differentiable modeling by using multi-component computation and introducing dynamic parameters (Feng et al., 2022). These so-called differentiable hydrologic models can achieve state-of-the-art hydrologic simulation accuracy tested in public benchmark dataset while also keep physical process clarity. Different from the purely machine learning methods, differentiable models can provide accurate simulations for a full set of untrained hydrologic variables. We further compared the extrapolation ability of different models (prediction in ungauged regions) in both continental and global scales. Differentiable models stand out showing superior spatial generalization performance compared with traditional parameter regionalization and purely machine learning methods (Feng et al., 2023; Feng et al., 2023). These characteristics show differentiable models can be great candidates as the next-generation global hydrologic model.</p>"},{"location":"codes/feng_2023/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{feng2022WRR,\nauthor = {Feng, Dapeng and Liu, Jiangtao and Lawson, Kathryn and Shen, Chaopeng},\ntitle = {Differentiable, Learnable, Regionalized Process-Based Models With Multiphysical Outputs can Approach State-Of-The-Art Hydrologic Prediction Accuracy},\njournal = {Water Resources Research},\nvolume = {58},\nnumber = {10},\npages = {e2022WR032404},\nkeywords = {rainfall runoff, differentiable programming, machine learning, physical model, differentiable hydrology, LSTM},\ndoi = {https://doi.org/10.1029/2022WR032404},\nyear = {2022}\n}\n@Article{feng2023HESS,\nAUTHOR = {Feng, D. and Beck, H. and Lawson, K. and Shen, C.},\nTITLE = {The suitability of differentiable, physics-informed machine learning\nhydrologic models for ungauged regions and climate change impact assessment},\nJOURNAL = {Hydrology and Earth System Sciences},\nVOLUME = {27},\nYEAR = {2023},\nNUMBER = {12},\nPAGES = {2357--2373},\nURL = {https://hess.copernicus.org/articles/27/2357/2023/},\nDOI = {10.5194/hess-27-2357-2023}\n}\n@article{feng2023GMDD,\n  title={Deep Dive into Global Hydrologic Simulations: Harnessing the Power of Deep Learning and Physics-informed Differentiable Models ($\\delta$HBV-globe1. 0-hydroDL)},\n  author={Feng, Dapeng and Beck, Hylke and de Bruijn, Jens and Sahu, Reetik Kumar and Satoh, Yusuke and Wada, Yoshihide and Liu, Jiangtao and Pan, Ming and Lawson, Kathryn and Shen, Chaopeng},\n  journal={Geoscientific Model Development Discussions},\n  volume={2023},\n  pages={1--23},\n  year={2023},\n  publisher={G{\\\"o}ttingen, Germany}\n}\n</code></pre>"},{"location":"codes/frameworks/","title":"\ud835\udeffMG","text":""},{"location":"codes/frameworks/#the-generic-scalable-differentiable-modeling-framework","title":"The Generic, Scalable Differentiable Modeling Framework","text":"<p>Code Release </p> <p><code>\ud835\udeffMG</code> is a domain-agnostic, PyTorch-based framework for developing trainable differentiable models that merge neural networks with process-based equations. \ud835\udeffMG is not a partcularly model. Rather, it is a generic framework that support many models across various domains (some are from HydroDL2.0) in a uniform way, while integrating ecosystem tools. Although the packages contains some basic examples for learners' convenience, the deployment models are supposed to exit in separate repositories and couple to the \ud835\udeffMG framework. </p> <p>For example, the <code>hydroDL2</code> repository for hydrology models couples with the \ud835\udeffMG framework to enable MHPI-specific hydrologic modeling capabilities. The combination serves both as a benchmark capability for published results (including those that used hydroDL) and an exploratory platform for future hydrology research in MHPI. </p> <p>We include an optional GUI (source code) for constructing/editing \ud835\udeffMG YAML configuration files with a user-friendly interface:</p> <p>Closely synergizes with deep learning tools and the scale advantage of PyTorch. Maintained by the MHPI group advised by Dr. Chaopeng Shen.</p> <p></p>"},{"location":"codes/frameworks/#differentiable-models","title":"Differentiable Models","text":"<p>Characterized by the combination of process-based equations with neural networks (NNs), differentiable models train these components together, enabling parameter inputs for the equations to be effectively and efficiently learned at scale by the NNs. There are many possibilities for how such models are built.</p> <p>In \ud835\udeffMG, we define a differentiable model with the class DeltaModel that can couple one or more NNs with a process-based model (itself potentially a collection of models). This class holds <code>nn</code> and a <code>phy_model</code> objects, respectively, as attributes internally and describes how they interface with each other:</p> <ul> <li>nn: PyTorch neural networks that can learn and provide either parameters, missing process representations, corrections, or other forms of enhancements to physical models.</li> <li>phy_model: The physical model written in PyTorch (or potentially another interoperable differentiable platform) that takes learnable outputs from the <code>nn</code> model(s) and returns a prediction of some target variable(s). This can also be a wrapper holding several physical models.</li> </ul> <p>The DeltaModel object can be trained and forwarded just as any other PyTorch model (nn.Module).</p>"},{"location":"codes/liu_2022/","title":"Multiscale","text":""},{"location":"codes/liu_2022/#code-release","title":"Code Release","text":"<p>zenodo version</p>"},{"location":"codes/liu_2022/#short-summary","title":"Short Summary","text":"<p>Here we propose a novel multiscale DL scheme learning simultaneously from satellite and in situ data to predict 9 km daily soil moisture (5 cm depth). Based on spatial cross-validation over sites in the conterminous United States, the multiscale scheme obtained a median correlation of 0.901 and root-mean-square error of 0.034 m3/m3.</p>"},{"location":"codes/liu_2022/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{liu2022multiscale,\n  title={A multiscale deep learning model for soil moisture integrating satellite and in situ data},\n  author={Liu, Jiangtao and Rahmani, Farshid and Lawson, Kathryn and Shen, Chaopeng},\n  journal={Geophysical Research Letters},\n  volume={49},\n  number={7},\n  pages={e2021GL096847},\n  year={2022},\n  publisher={Wiley Online Library}\n}\n</code></pre>"},{"location":"codes/liu_2024/","title":"Transformer","text":""},{"location":"codes/liu_2024/#code-release","title":"Code Release","text":"<p>zenodo version</p>"},{"location":"codes/liu_2024/#short-summary","title":"Short Summary","text":"<p>This repository contains the code for our paper published in the Journal of Hydrology: Probing the limit of hydrologic predictability with the Transformer network</p>"},{"location":"codes/liu_2024/#bibtex-citation","title":"Bibtex Citation","text":"<pre><code>@article{LIU2024131389,\n  title = {Probing the limit of hydrologic predictability with the Transformer network},\n  journal = {Journal of Hydrology},\n  volume = {637},\n  pages = {131389},\n  year = {2024},\n  issn = {0022-1694},\n  doi = {https://doi.org/10.1016/j.jhydrol.2024.131389},\n  url = {https://www.sciencedirect.com/science/article/pii/S0022169424007844},\n  author = {Jiangtao Liu and Yuchen Bian and Kathryn Lawson and Chaopeng Shen}\n}\n</code></pre>"},{"location":"datasets/CONUS/","title":"A 40-year continental Hydrologic dataset on seamless MERIT river network and ~180,000 MERIT unit basins","text":""},{"location":"datasets/CONUS/#paper","title":"Paper","text":"<p>Song, Y., Bindas, T., Shen, C., Ji, H., Knoben, W. J. M., Lonzarich, L., et al. (2025). High\u2010resolution national\u2010scale water modeling is enhanced by multiscale differentiable physics\u2010informed machine learning. Water Resources Research, 61, e2024WR038928. https://doi.org/10.1029/2024WR038928</p>"},{"location":"datasets/CONUS/#results","title":"Results","text":""},{"location":"datasets/CONUS/#data-description","title":"Data description","text":"<p>\ud835\udeffHBV2_0_continental_data (zenodo and Colab code for data processing) is from High-resolution, multiscale, differentiable HBV hydrologic models, \ud835\udeffHBV2.0UH and \ud835\udeffHBV2.0dMC. \ud835\udeffHBV2.0UH is a high-resolution, multiscale model that uses unit hydrograph routing for. \ud835\udeffHBV2.0dMC is a high-resolution, multiscale model that uses external Muskingum-Cunge routing.</p> <p>The dHBV_streamflow_simulation_gages folder includes 40 years (1980\u20132020) of streamflow simulations at over 7,000 gage stations from GAGES-II, using both \ud835\udeffHBV2.0UH and \ud835\udeffHBV2.0dMC models. This data is useful for comparison with observations. The MERIT_flux_states folder includes 40 years (1980\u20132020) of spatially seamless simulations of hydrologic variables over 180 thousand MERIT unit basins on CONUS from \ud835\udeffHBV2.0UH, including baseflow, evapotranspiration (ET), soil moisture, snow water equivalent, and runoff. The dHBV2.0_MERIT_river_network_simulation includes 40 years (1980-2020) of streamflow simulation on seamless MERIT river network by dHBV2.0dMC (New updates!).</p>"},{"location":"datasets/CONUS/#code-release","title":"Code Release","text":"<p>The dHBV2.0UH code is available at mhpi/generic_deltaModel: High-resolution differentiable model, \ud835\udeffHBV2.0. https://doi.org/10.5281/zenodo.14827983</p>"},{"location":"datasets/global/","title":"A seamless global streamflow dataset for ~2.94 million rivers from 1980-2020.","text":""},{"location":"datasets/global/#paper","title":"Paper","text":"<p>Ji, H., Song, Y., Bindas, T. et al. Distinct hydrologic response patterns and trends worldwide revealed by physics-embedded learning. Nat Commun 16, 9169 (2025). https://doi.org/10.1038/s41467-025-64367-1</p>"},{"location":"datasets/global/#results","title":"Results","text":"Metrics for the global dataset NSE KGE Bias RMSE FLV FHV 0.515 0.542 -0.023 5.995 22.791 -13.333"},{"location":"datasets/global/#data-description","title":"Data description","text":"<p>The global-scale seamless simulation from 1980-2020 zenodo is produced by a coupled High-resolution, multiscale, differentiable global water model that incorporates both a rainfall-runoff module (\ud835\udeffHBV2.0) and channel routing module (\u03b4MC2.0). To develop this high-resolution global dataset, we compiled 6,165 streamflow stations worldwide to train our model for the period 1980\u20132000 and conducted temporal validation from 2001 to 2015. The model achieved a median NSE of 0.721 and a median KGE of 0.725 during validation. Using the trained model, we then produced a seamless global simulation spanning 1980\u20132020. also, if you are intereseted in the near-real-time simulation, please let us know.</p>"},{"location":"datasets/global/#code-release","title":"Code Release","text":"<p>The dHBV2.0UH code is available at mhpi/generic_deltaModel: High-resolution differentiable model, \ud835\udeffHBV2.0. https://doi.org/10.5281/zenodo.14827983</p>"},{"location":"dmg/code/","title":"Code Release","text":""},{"location":"dmg/code/#getting-started","title":"Getting Started","text":"<p>To start using the \ud835\udeffMG framework, clone <code>generic_deltaModel</code> from GitHub:</p> <ul> <li>\ud835\udeffMG</li> </ul> <p>For differentiable hydrology tutorials and MHPI benchmarks, also clone the hydrologic model package <code>hydroDL2</code> and download our CAMELS data extraction:</p> <ul> <li>HydroDL 2.0</li> <li>CAMELS Data</li> </ul>"},{"location":"dmg/code/#nextgen","title":"NextGen","text":"<p>Code supporting the high-resolution, national-scale differentiable water modeling product, \ud835\udeffHBV2.0 (Song et al., 2025), in NOAA's NextGen water modeling framework can be found here:</p> <ul> <li>\ud835\udeffHBV2.0: Physical HBV2.0 model with BMI, configurations, and realization files supporting \ud835\udeffHBV2.0 operation in NextGen.</li> <li>NextGen: NextGen framework including the \ud835\udeffHBV2.0 module above.</li> <li>NGIAB: NextGen In A Box using \ud835\udeffHBV2.0 and the NextGen version above.</li> </ul> <p>Instructions for setup in NextGen can be found in the \ud835\udeffHBV2.0 repo, and for instructions for NGIAB can be found in the repo or the official DocuHub.</p>"},{"location":"dmg/detail/","title":"\ud835\udeffMG","text":"<p>A generic framework for building differentiable models. \ud835\udeffMG enables seamless coupling of neural networks with differentiable process-based equations, leveraging PyTorch's auto-differentiation for efficient, large-scale optimization on GPU. The spiritual successor to HydroDL, \ud835\udeffMG generalizes differentiable modeling for cross-domain application while also imposing basic standardizations for research-to-operations pipelines.</p>"},{"location":"dmg/detail/#key-features","title":"Key Features","text":"<ul> <li>\ud83e\udd1d Hybrid Modeling: Combine neural networks with process-based equations for enhanced interpretability and generalizability. Instead of manual model parameter calibration, for instance, use neural networks to directly learn robust and interpretable parameters (Tsai et al., 2021).</li> <li>\ud83d\udd01 PyTorch Integration: Scale with PyTorch for efficient training and compatibility with modern ML tools and numerical solvers.</li> <li>\ud83e\udde9 Modular Plugin Architecture: Swap in domain-specific components and configurations with ease.</li> <li>\u26a1 Benchmarking: All in one place. \ud835\udeffMG + hydroDL2 will enable rapid deployment and replication of key published MHPI results.</li> <li>\ud83c\udf0a NextGen-ready: Designed for CSDMS BMI compliance to support differentiable hydrological models in NOAA-OWP's NextGen National Water Modeling Framework. (See the NextGen-ready \ud835\udeffHBV2.0 for an example with a \ud835\udeffMG-supported BMI).</li> </ul> <p>\ud835\udeffMG is designed to scale with modern deep learning tools (e.g., foundation models) while maintaining physical interpretability. Our peer-reviewed and published benchmarks show that well-tuned differentiable models can match deep networks in performance\u2014while better extrapolating to extreme or data-scarce conditions and predicting physically meaningful variables.</p> <p>Differentiable modeling introduces more modeling choices than traditional deep learning due to its physical constraints. This includes learning parameters, missing process representations, corrections, or other enhancements for physical models.</p> <p>Note: While differentiable models are powerful and have many desirable characteristics, they come with a larger decision space than purely data-driven neural networks since physical processes are involved, and can thus feel \"trickier\" to work with. Hence, we recommend beginning with our example notebooks and then systematically making changes, one at a time. Pay attention to multifaceted outputs, diverse causal analyses, and predictions of untrained variables permitted by differentiable models, rather than purely trying to outperform other models' metrics.</p> <p></p> <p>This work is mantained by MHPI and advised by Dr. Chaopeng Shen. If you find it useful, please cite (dedicated citations are coming):</p> <pre><code>Shen, C., et al. (2023). Differentiable modelling to unify machine learning and physical models for geosciences. Nature Reviews Earth &amp; Environment, 4(8), 552\u2013567. &lt;https://doi.org/10.1038/s43017-023-00450-9&gt;.\n</code></pre> <p></p>"},{"location":"dmg/detail/#installation","title":"Installation","text":"<p>To install \ud835\udeffMG, clone the repo and install in developer mode with Astral UV:</p> <pre><code>git clone git@github.com:mhpi/generic_deltamodel.git\nuv pip install -e ./generic_deltamodel\n</code></pre> <p>Pip and Conda are also supported, though UV is recommended. See setup for further details.</p> <p></p>"},{"location":"dmg/detail/#quick-start","title":"Quick Start","text":"<p>See how to run.</p> <p>Example -- Differentiable Parameter Learning: Use an LSTM to learn parameters for the HBV hydrological model.</p> <pre><code>from hydroDL2.models.hbv.hbv import HBV\n\nfrom dMG.core.data.loaders import HydroLoader\nfrom dMG.core.utils import load_nn_model, print_config, set_randomseed\nfrom dMG.models.delta_models import DplModel\nfrom example import load_config, take_data_sample\n\nCONFIG_PATH = '../example/conf/config_dhbv_1_0.yaml'\n\n\n# Model configuration\nconfig = load_config(CONFIG_PATH)\n\n# Initialize physical model and NN.\nphy_model = HBV(config['delta_model']['phy_model'])\nnn = load_nn_model(phy_model, config['delta_model'])\n\n# Create differentiable model dHBV: a torch.nn.Module that describes how \n# the NN is linked to the physical model HBV.\ndpl_model = DplModel(phy_model=phy_model, nn_model=nn)\n\n# Load dataset of NN and HBV inputs.\ndataset = HydroLoader(config).dataset\ndataset_sample = take_data_sample(config, dataset, days=730, basins=100)\n\noutput = dpl_model(dataset_sample)\n</code></pre> <p>This exposes a key characteristic of the differentiable model <code>DplModel</code>: composition of a physical model, <code>phy_model</code>, and a neural network, <code>nn</code>. Internally, <code>DplModel</code> looks like</p> <pre><code># NN forward\nparameters = self.nn_model(dataset_sample['xc_nn_norm'])        \n\n# Physics model forward\npredictions = self.phy_model(\n    dataset_sample,\n    parameters,\n)\n</code></pre> <p>Check out examples to see model training/testing/simulation in detail. We recommend starting here, which is a continuation of the above. A Colab Notebook for this \u03b4HBV example is also available.</p> <p></p>"},{"location":"dmg/detail/#use-cases","title":"Use Cases","text":""},{"location":"dmg/detail/#1-lumped-hydrology","title":"1. Lumped Hydrology","text":"<p>Lumped differentiable rainfall-runoff models \ud835\udeffHBV 1.0 and improved \ud835\udeffHBV 1.1p.</p>"},{"location":"dmg/detail/#2-unseen-extreme-events-test-with-hbv-11p","title":"2. Unseen Extreme Events Test with \ud835\udeffHBV 1.1p","text":"<p>In the unseen extreme events spatial test, we used water years with a 5-year or lower return period peak flow from 1990/10/01 to 2014/09/30 for training, and held out the water years with greater than a 5-year return period peak flow for testing. The spatial test was conducted using a 5-fold cross-validation approach for basins in the CAMELS dataset. This application has been benchmarked against LSTM and demonstrates better extrapolation abilities. Find more details and results in Song, Sawadekar, et al. (2024).</p>"},{"location":"dmg/detail/#3-national-scale-water-modeling","title":"3. National-scale Water Modeling","text":"<p>A national-scale water modeling study on approximately 180,000 river reaches (with a median length of 7 km) across CONUS using the high-resolution, multiscale, differentiable water model \ud835\udeffHBV 2.0. This model is also operating at global-scales and has been used to generate high-quality, seamless simulations for the entire CONUS. Find more details and results in Song, Bindas, et al. (2025).</p>"},{"location":"dmg/detail/#4-global-scale-photosynthesis-modeling","title":"4. Global-scale Photosynthesis Modeling","text":"<p>Currently in development. Find more details and results in Aboelyazeed et al. (2024).</p> <p></p>"},{"location":"dmg/detail/#ecosystem-integration","title":"Ecosystem Integration","text":"<ul> <li>HydroDL 2.0 (<code>hydroDL2</code>): Home to MHPI's suite of process-based hydrology models and differentiable model augmentations.</li> </ul> <ul> <li>Differentiable Ecosystem Modeling (<code>diffEcosys (dev version only)</code>): A physics-informed machine learning system for ecosystem modeling, demonstrated using the photosynthesis process representation within the Functionally Assembled Terrestrial Ecosystem Simulator (FATES) model. This model is coupled to NNs that learn parameters from observations of photosynthesis rates.</li> <li>Other Development: Many additions are currently in the progress: (i) numerical PDE solvers on PyTorch, torchode, torchdiffeq; (ii) adjoint sensitivity; (iii) extremely efficient and highly accurate surrogate models for process-based equations; (iv) data assimilation methods; (v) downscaled and bias-corrected climate data; (vi) mysteriously powerful neural networks, and more ...</li> </ul>"},{"location":"dmg/detail/#mg-architecture","title":"\ud835\udeffMG Architecture","text":"<ul> <li>Data Loaders: Bulk data preprocessors customized per dataset.</li> <li>Data Samplers: Dataset samplers for minibatching during model training and inference.</li> <li>Trainers: Orchestrates high-level model training, testing, and simulation.</li> <li>ModelHandler: Manages multimodeling, multi-GPU computation, and other high level operations. Acts as an drop-in model interface for CSDMS BMI or other R2O wrappers.</li> <li>Delta Models: Differentiable models; describes how NNs and physical models are coupled (e.g., <code>DplModel</code> for parameter learning).</li> </ul>"},{"location":"dmg/detail/#repo","title":"Repo","text":"<pre><code>```text\n.\n\u251c\u2500\u2500 src/dMG/\n\u2502   \u251c\u2500\u2500 __main__.py                 # Runs \ud835\udeffMG; models, experiments\n\u2502   \u251c\u2500\u2500 core/                       \n\u2502   \u2502   \u251c\u2500\u2500 calc/                   # Calculation utilities\n\u2502   \u2502   \u251c\u2500\u2500 data/                   # Data loaders and samplers\n\u2502   \u2502   \u251c\u2500\u2500 post/                   # Post-processing utilities; plotting\n\u2502   \u2502   \u2514\u2500\u2500 utils/                  # Helper functions\n\u2502   \u251c\u2500\u2500 models/                     \n\u2502   \u2502   \u251c\u2500\u2500 criterion               # Loss functions  \n\u2502   \u2502   \u251c\u2500\u2500 delta_models            # Differentiable model modalities\n\u2502   \u2502   \u251c\u2500\u2500 multimodels             # Multimodeling processors\n\u2502   \u2502   \u251c\u2500\u2500 neural_networks/        # Neural network architectures\n\u2502   \u2502   \u251c\u2500\u2500 phy_models/             # Physical Models\n\u2502   \u2502   \u2514\u2500\u2500 model_handler.py        # High-level model manager\n\u2502   \u2514\u2500\u2500 trainers/                   # Model training routines\n\u251c\u2500\u2500 conf/\n\u2502   \u251c\u2500\u2500 hydra/                      # Hydra settings\n\u2502   \u251c\u2500\u2500 observations/               # Observation configuration files\n\u2502   \u251c\u2500\u2500 config.py                   # Configuration validator\n\u2502   \u2514\u2500\u2500 default.yaml                # Default master configuration file\n\u251c\u2500\u2500 docs/                           \n\u251c\u2500\u2500 envs/                           # Python ENV configurations\n\u2514\u2500\u2500 example/                        # Tutorials\n```\n</code></pre>"},{"location":"dmg/detail/#contributing","title":"Contributing","text":"<p>We welcome contributions! Please submit changes via a fork and pull requests. For more details, refer to docs/CONTRIBUTING.md.</p> <p>Please submit an issue to report any questions, concerns, bugs, etc.</p>"},{"location":"docs/","title":"Docs","text":"<p>HydroDL's differentiable modeling interface is set up to provide a seemless template for using physics models and neural networks together. </p> <p>Our framework provides a standard for differentiable physics models to follow such that anyone can plug their models together. </p>"},{"location":"docs/#sections","title":"Sections","text":"<ul> <li>Datasets</li> <li>Neural Networks</li> <li>Physics Models</li> <li>Experiments</li> <li>Configs</li> </ul>"},{"location":"docs/#plugins","title":"Plugins","text":"<p>Plugins are a way to build off of open-source Deep Learning papers and repositories. </p>"},{"location":"docs/datasets/","title":"Datasets","text":"<p>The Datasets used in hydroDL are individual <code>@dataclass</code> classes used to create a Pytorch <code>torch.utils.data.Dataloader</code>. The classe</p> <code>Data</code> <p>Inputs to the neural networks</p> <code>Observations</code> <p>Targets used when training</p>"},{"location":"docs/datasets/#data","title":"Data","text":"<p>Data classes are implementations of the following <code>ABC</code> class:</p> __init__.Data.py<pre><code>from abc import ABC, abstractmethod\n\nfrom omegaconf import DictConfig\nimport torch\n\nclass Data(ABC):\n     @abstractmethod\n    def __init__(self, cfg: DictConfig, dates: Dates, normalize: Normalize):\n        \"\"\"A function to define what inputs are required by a Data object\"\"\"\n        pass\n\n    @abstractmethod\n    def _read_attributes(self) -&gt; None:\n        \"\"\"\n        Abstract method for reading attributes related to the data.\n\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _read_forcings(self) -&gt; None:\n        \"\"\"\n        Abstract method for reading attributes related to the data.\n        :return: None\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _read_data(self) -&gt; None:\n        \"\"\"The method to read all data\"\"\"\n        pass\n\n    @abstractmethod\n    def get_data(self) -&gt; Hydrofabric:\n        \"\"\"\n        Abstract method for retrieving data in the form of a hydrofabric\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"docs/experiments/","title":"Experiments","text":"<p>HydroDL experiments are designed to seamlessly be both reusableand structured. All experiments are child classes of the base <code>Experiment</code> class:</p> __init__.Experiment.py<pre><code>from abc import ABC, abstractmethod\nfrom typing import Dict, Type\n\nimport torch\nimport torch.nn\n\nclass Experiment(ABC):\n    @abstractmethod\n    def run(\n        self,\n        data_loader: torch.utils.data.DataLoader,\n        neural_network: nn.Module,\n        physics_models: Dict[str, Type[nn.Module]],\n    ) -&gt; None:\n        \"\"\"a method that runs your experiment\"\"\"\n        pass\n</code></pre> <p>The arguments passed into the parameters of the run function are all either class references (<code>physics_models</code>) or full instantiated classes (<code>data_loader</code>, or <code>neural_network</code>)</p>"},{"location":"docs/neural_networks/","title":"Neural Networks","text":"<p>Neural Networks are configured similar to how they are instantiated in other PyTorch packages. </p> <pre><code>from functools import partial\n\nfrom omegaconf import DictConfig\nimport torch\nimport torch.nn as nn\n\nfrom hydroRoute.neural_networks import Initialization\n\nclass NN(nn.Module):\n    def __init__(self, cfg: DictConfig):\n        super(MLP, self).__init__()\n        self.cfg = cfg\n        self.Initialization = Initialization(self.cfg)\n\n    def initialize_weights(self) -&gt; None:\n        \"\"\"\n        The partial function used to \n        \"\"\"\n        func = self.Initialization.get()\n        init_func = partial(self._initialize_weights, func=func)\n        self.apply(init_func)\n\n    @staticmethod\n    def _initialize_weights(m, func) -&gt; None:\n        \"\"\"\n        An internal class used to intialize weights based\n        on a provided initialization function\n        \"\"\"\n        if isinstance(m, nn.Linear):\n            func(m.weight)\n\n    def forward(self, inputs: torch.Tensor) -&gt; None:\n        pass\n</code></pre>"},{"location":"docs/physics_models/","title":"Physics Models","text":"<p>HydroDL's implemented physics models are all child classes of the Pytorch <code>nn.Module</code> class. By creating your physics model as an nn.Module, you can tap into PyTorch's neural network functionality and get a lot of bonuses. </p>"},{"location":"docs/physics_models/#basics","title":"Basics","text":"<p>Our physics models are structured as follows:</p> <pre><code>from typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nclass PhysicsModel(nn.Module):\n    def __init__(self, cfg: DictConfig) -&gt; None:\n        super(PhysicsModel, self).__init__()\n        self.cfg = cfg\n\n    def forward(self, inputs: Tuple[..., torch.Tensor]) -&gt; torch.Tensor:\n</code></pre> <p>Where all that is required by a physics model is it's specified configuration file. Since there are different requirements for each physics model, it is necessary for you to read into the specific configurations required by each module. </p>"},{"location":"docs/plugins/","title":"Plugins","text":""},{"location":"docs/plugins/hydrodl/","title":"HydroDL","text":""}]}